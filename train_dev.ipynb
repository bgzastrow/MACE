{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib        as mpl\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn          as nn\n",
    "from torch.utils.data    import Dataset, DataLoader\n",
    "from torch.optim         import Adam\n",
    "\n",
    "from matplotlib          import rcParams, rc\n",
    "rcParams.update({'figure.dpi': 200})\n",
    "mpl.rcParams.update({'font.size': 8})\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "\n",
    "## import own functions\n",
    "sys.path.insert(1, '/lhome/silkem/MACE/MACE/src/mace')\n",
    "import autoencoder  as ae\n",
    "import dataset      as ds\n",
    "import plotting     as pl\n",
    "import train        as tr\n",
    "import neuralODE    as nODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up PyTorch \n",
    "cuda   = False\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "batch_size = 1 ## if not 1, dan kan er geen tensor van gemaakt worden\n",
    "epochs = 1\n",
    "lr = 0.01\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "------------------------------\n",
      "total # of samples: 7569\n",
      "# training samples: 5298\n",
      "# testing samples:  2271\n",
      "            ratio:  0.3\n"
     ]
    }
   ],
   "source": [
    "dirname = 'easy-mace'\n",
    "\n",
    "train, data_loader, test_loader = ds.get_data(dirname = dirname, batch_size=batch_size, kwargs=kwargs, plot = True, scale = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nODE.Solver(p_dim=4,z_dim = 10, n_dim=466)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:         \n",
      "learning rate: 0.01\n",
      "\n",
      " >>> Training model...\n",
      "train\n",
      "5295\n",
      " test\n",
      "0\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x106 and 466x68)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## Training & validating model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m loss_train_all, loss_test_all \u001b[39m=\u001b[39m tr\u001b[39m.\u001b[39;49mtrain(model, lr, data_loader, test_loader, epochs, DEVICE)\n\u001b[1;32m      4\u001b[0m x_test, x_test_hat, loss \u001b[39m=\u001b[39m tr\u001b[39m.\u001b[39mtest(model, test_loader, DEVICE)\n",
      "File \u001b[0;32m~/MACE/MACE/src/mace/train.py:110\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, lr, data_loader, test_loader, epochs, DEVICE, plot, log, show)\u001b[0m\n\u001b[1;32m    107\u001b[0m model\u001b[39m.\u001b[39meval() \u001b[39m## zelfde als torch.no_grad\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m test\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m test_loss \u001b[39m=\u001b[39m validate_one_epoch(test_loader, model, DEVICE)\n\u001b[1;32m    111\u001b[0m loss_test_all\u001b[39m.\u001b[39mappend(test_loss)\n\u001b[1;32m    113\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mEpoch\u001b[39m\u001b[39m\"\u001b[39m, epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcomplete!\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mAverage loss train: \u001b[39m\u001b[39m\"\u001b[39m, train_loss, \u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mAverage loss test: \u001b[39m\u001b[39m\"\u001b[39m, test_loss, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/MACE/MACE/src/mace/train.py:79\u001b[0m, in \u001b[0;36mvalidate_one_epoch\u001b[0;34m(test_loader, model, DEVICE)\u001b[0m\n\u001b[1;32m     76\u001b[0m p     \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mto(DEVICE) \n\u001b[1;32m     77\u001b[0m t     \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 79\u001b[0m n_hat \u001b[39m=\u001b[39m model(n[:,\u001b[39m0\u001b[39;49m,:],p,t)         \u001b[39m## output van het autoecoder model\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m## Calculate losses\u001b[39;00m\n\u001b[1;32m     82\u001b[0m loss  \u001b[39m=\u001b[39m loss_function(n,n_hat)\n",
      "File \u001b[0;32m~/anaconda3/envs/mace/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/MACE/MACE/src/mace/neuralODE.py:113\u001b[0m, in \u001b[0;36mSolver.forward\u001b[0;34m(self, n_0, p, tstep)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, n_0, p, tstep):\n\u001b[0;32m--> 113\u001b[0m     z_0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(n_0)\n\u001b[1;32m    115\u001b[0m     problem \u001b[39m=\u001b[39m to\u001b[39m.\u001b[39mInitialValueProblem(\n\u001b[1;32m    116\u001b[0m         y0     \u001b[39m=\u001b[39m z_0\u001b[39m.\u001b[39mview((\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)),  \u001b[39m## \"view\" is om met de batches om te gaan\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         t_eval \u001b[39m=\u001b[39m tstep\u001b[39m.\u001b[39mview((\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)),\n\u001b[1;32m    118\u001b[0m     )\n\u001b[1;32m    120\u001b[0m     solution \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit_solver\u001b[39m.\u001b[39msolve(problem, args\u001b[39m=\u001b[39mp)\n",
      "File \u001b[0;32m~/anaconda3/envs/mace/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/MACE/MACE/src/mace/autoencoder.py:34\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 34\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLeakyReLU(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_in(x))\n\u001b[1;32m     35\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden:\n\u001b[1;32m     36\u001b[0m         h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLeakyReLU(layer(h))\n",
      "File \u001b[0;32m~/anaconda3/envs/mace/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mace/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x106 and 466x68)"
     ]
    }
   ],
   "source": [
    "## Training & validating model\n",
    "\n",
    "loss_train_all, loss_test_all = tr.train(model, lr, data_loader, test_loader, epochs, DEVICE)\n",
    "x_test, x_test_hat, loss = tr.test(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1054, 1.1729])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(2,2).ravel()\n",
    "B = torch.rand(2,2,2).ravel()\n",
    "# print(A)\n",
    "\n",
    "A = A.reshape(2,2)\n",
    "B = B.reshape(2,2,2)\n",
    "\n",
    "z = torch.rand(2)\n",
    "\n",
    "\n",
    "torch.einsum(\"ij, j -> i\", A, z) + torch.einsum(\"ijk, j, k -> i\", B, z, z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
